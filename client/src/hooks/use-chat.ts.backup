import { useState, useCallback, useRef } from "react";
import { useQuery, useMutation, useQueryClient } from "@tanstack/react-query";
import { apiRequest } from "@/lib/queryClient";
import type { ChatMessage, Prompt } from "@shared/schema";
import { useToast } from "@/hooks/use-toast";
import { track } from "@/lib/telemetry";
// Azure Speech SDK is dynamically imported when needed to keep initial bundle fast

export function useChat() {
  const [error, setError] = useState<string | null>(null);
  const queryClient = useQueryClient();
  const sessionId = localStorage.getItem('sessionId');
  const { toast } = useToast();
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const [visemes, setVisemes] = useState<{ id: number; offset: number }[]>([]);
  const [visemeStartTime, setVisemeStartTime] = useState<number | null>(null);
  const synthesizerRef = useRef<any | null>(null);

  // Granular loading states
  const [isAiProcessing, setIsAiProcessing] = useState(false);
  const [isTtsProcessing, setIsTtsProcessing] = useState(false);

  // Config: whether to delay showing text until audio starts (strict sync)
  const SYNC_TEXT_TO_AUDIO = (import.meta as any).env?.VITE_SYNC_TEXT_TO_AUDIO === 'true';

  // Fetch existing chat messages
  const { data: messages = [], isLoading: isLoadingMessages } = useQuery<ChatMessage[]>({
    queryKey: ['/api/chat/messages', sessionId],
    queryFn: async () => {
      if (!sessionId) return [];
      const response = await apiRequest('GET', `/api/chat/messages?sessionId=${sessionId}`);
      return response.json();
    },
    staleTime: 0, // Always fetch fresh for chat
    enabled: !!sessionId, // Only run the query if sessionId exists
  });

  // Send message mutation
  const sendMessageMutation = useMutation({
    mutationFn: async ({ message, prompts }: { message: string, prompts: Prompt[] }) => {
      if (!sessionId) throw new Error("Session ID not found");
      const response = await apiRequest('POST', '/api/chat', { message, prompts, sessionId });
      return response.json();
    },
    onMutate: () => {
      setIsAiProcessing(true);
      setError(null);
    },
    onSuccess: (data: { chatMessage: ChatMessage, audioContent: string | null, ttsError?: string | null }) => {
      setIsAiProcessing(false);
      setIsTtsProcessing(true);
      const { chatMessage, audioContent, ttsError } = data;
      // Helper to add message to cache (either immediately or aligned with audio start)
      const addMessageToCache = () => {
        queryClient.setQueryData<ChatMessage[]>(['/api/chat/messages', sessionId], (oldMessages = []) => {
          return [...oldMessages, chatMessage];
        });
      };
      setError(null);

      // Display TTS error as a toast notification if it exists
      if (ttsError) {
        toast({
          title: "TTS Error",
          description: ttsError,
          variant: "destructive",
        });
      }

      // Handle server-side TTS audio and visemes
      if (audioContent && chatMessage.response) {
        try {
          // Show text early for responsiveness unless strict sync is requested
          if (!SYNC_TEXT_TO_AUDIO) {
            addMessageToCache();
          }

          // Convert base64 audio to ArrayBuffer if needed, or use directly
          let audioBuffer: ArrayBuffer;
          if (typeof audioContent === 'string') {
            // Assume base64 encoded audio
            const binaryString = atob(audioContent);
            audioBuffer = new ArrayBuffer(binaryString.length);
            const bytes = new Uint8Array(audioBuffer);
            for (let i = 0; i < binaryString.length; i++) {
              bytes[i] = binaryString.charCodeAt(i);
            }
          } else {
            audioBuffer = audioContent;
          }

          // Get visemes from server metadata
          const metadata = chatMessage.metadata ? JSON.parse(chatMessage.metadata) : {};
          const serverVisemes = metadata.visemes || [];

          // Play audio and sync visemes
          const audioContext = new AudioContext();
          audioContext.decodeAudioData(audioBuffer.slice(0), (buffer) => {
            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);

            // Set visemes and start time
            setVisemes(serverVisemes);
            setVisemeStartTime(Date.now());

            if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
              addMessageToCache();
              messageAdded = true;
            }

            source.onended = () => {
              setVisemes([]);
              setVisemeStartTime(null);
              setIsTtsProcessing(false);
            };

            source.start(0);
          }, (error) => {
            console.error('Audio decode error:', error);
            setIsTtsProcessing(false);
            if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
              addMessageToCache();
              messageAdded = true;
            }
          });

        } catch (e) {
          console.error("Error playing server audio:", e);
          setIsTtsProcessing(false);
          if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
            addMessageToCache();
            messageAdded = true;
          }
        }
      } else {
        // No audio available, just show text
        setIsTtsProcessing(false);
        addMessageToCache();
      }
    },
    onError: (err: Error) => {
            for (let attempt = 1; attempt <= maxRetries; attempt++) {
              try {
                const synthesizer = new sdk.SpeechSynthesizer(speechConfig, audioConfig);
                synthesizerRef.current = synthesizer;

                return await new Promise((resolve, reject) => {
                  synthesizer.speakTextAsync(text, (result: any) => {
                    if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {
                      resolve(result);
                    } else {
                      reject(new Error(`Synthesis failed: ${result.errorDetails || 'Unknown error'}`));
                    }
                  });
                });
              } catch (error) {
                console.warn(`TTS attempt ${attempt} failed:`, error);
                if (attempt === maxRetries) throw error;
                // Wait a bit before retrying
                await new Promise(resolve => setTimeout(resolve, 1000));
              }
            }
          };

          // Dynamically import the SDK
          const loadSdk = () => import('microsoft-cognitiveservices-speech-sdk');
          loadSdk().then((sdk) => {
            try {
              const speechConfig = sdk.SpeechConfig.fromSubscription(import.meta.env.VITE_AZURE_TTS_KEY, import.meta.env.VITE_AZURE_REGION);
              speechConfig.speechSynthesisVoiceName = 'en-US-ChristopherNeural';

              // Create a controllable speaker destination so we know EXACTLY when audio playback starts
              const player = new sdk.SpeakerAudioDestination();
              const audioConfig = sdk.AudioConfig.fromSpeakerOutput(player);
              const synthesizer = new sdk.SpeechSynthesizer(speechConfig, audioConfig);
              synthesizerRef.current = synthesizer;

              const visemeList: { id: number; offset: number }[] = [];
              let messageAdded = false;

              synthesizer.visemeReceived = (_s: any, e: any) => {
                // e.audioOffset is in 100-nanosecond units; convert to milliseconds
                visemeList.push({ id: e.visemeId, offset: e.audioOffset / 10000 });
              };

              // Align UI text, visemes, and audio start together
              player.onAudioStart = () => {
                setVisemes(visemeList);
                setVisemeStartTime(Date.now());
                if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
                  addMessageToCache();
                  messageAdded = true;
                }
              };
              player.onAudioEnd = () => {
                // Clear after speech finishes
                setVisemes([]);
                setVisemeStartTime(null);
                setIsTtsProcessing(false);
                if (synthesizerRef.current === synthesizer) {
                  synthesizerRef.current = null;
                }
              };

              console.log('Starting Azure TTS synthesis with retry');
              const speakWithRetry = async (text: string) => {
                let lastError: any = null;
                for (let attempt = 1; attempt <= 2; attempt++) {
                  try {
                    return await new Promise((resolve, reject) => {
                      synthesizer.speakTextAsync(text, (result: any) => {
                        if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {
                          resolve(result);
                        } else {
                          reject(new Error(result.errorDetails || 'Synthesis failed'));
                        }
                      });
                    });
                  } catch (error) {
                    console.warn(`TTS attempt ${attempt} failed:`, error);
                    lastError = error;
                    if (attempt < 2) {
                      await new Promise(resolve => setTimeout(resolve, 1000)); // Wait 1 second before retry
                    }
                  }
                }
                throw lastError;
              };

              speakWithRetry(chatMessage.response!).then(() => {
                console.log('Azure TTS synthesis completed successfully');
                synthesizer.close();
                // If, for any reason, audio start didn't fire, ensure text appears
                setTimeout(() => {
                  if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
                    addMessageToCache();
                    messageAdded = true;
                  }
                }, 200);
              }).catch((error) => {
                console.error("Speech synthesis failed after retries:", error);
                synthesizer.close();
                track('tts_error', { reason: String(error) });
                toast({
                  title: "TTS Error",
                  description: "Failed to synthesize speech after retries.",
                  variant: "destructive",
                });
                if (SYNC_TEXT_TO_AUDIO && !messageAdded) {
                  addMessageToCache();
                  messageAdded = true;
                }
              });
            } catch (sdkErr) {
              console.error("Speech SDK use error:", sdkErr);
              addMessageToCache();
            }
          }).catch((importErr) => {
            console.error("Failed to import speech SDK:", importErr);
            track('tts_error', { reason: 'sdk_import_failed', error: String(importErr) });
            addMessageToCache();
          });
        } catch (e) {
          console.error("Error initializing speech synthesizer:", e);
          track('tts_error', { reason: 'init_error', error: String(e) });
          toast({
            title: "TTS Error",
            description: "Could not initialize text-to-speech.",
            variant: "destructive",
          });
          // Fall back to showing text immediately
          addMessageToCache();
        }
      } else if (audioContent) {
        // No Azure TTS available - show text immediately
        addMessageToCache();
      }
    },
    onError: (err: Error) => {
      setIsAiProcessing(false);
      setIsTtsProcessing(false);
      setError(err.message || 'Failed to send message');
      toast({
        title: "Error",
        description: err.message || 'Failed to send message',
        variant: "destructive",
      });
    },
  });

  const sendMessage = useCallback(async (message: string, prompts: Prompt[]) => {
    setError(null);
    return sendMessageMutation.mutateAsync({ message, prompts });
  }, [sendMessageMutation]);

  return {
    messages,
    sendMessage,
    isLoading: sendMessageMutation.isPending,
    isAiProcessing,
    isTtsProcessing,
    isLoadingMessages,
    error: error || (sendMessageMutation.error?.message),
    visemes,
    visemeStartTime,
  };
}